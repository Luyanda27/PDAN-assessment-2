{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f7c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INITIALIZATION: IMPORTING NECESSARY LIBRARIES ---\n",
    "# Fundamental libraries for data manipulation and numerical computation.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Libraries for data visualization.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Scikit-learn modules for preprocessing, data splitting, and model evaluation.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_recall_curve, auc\n",
    "# Statsmodels provides detailed statistical output for feature selection.\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# --- PHASE 1: EXPLORATORY DATA ANALYSIS AND CLEANSING ---\n",
    "\n",
    "# Load the dataset from the source file.\n",
    "df = pd.read_csv('goodreads_books_clean.csv')\n",
    "\n",
    "# Conduct an initial inspection to understand data structure and integrity.\n",
    "print(\"Dataset Dimensionality:\", df.shape)\n",
    "print(\"\\nData Types and Null Counts:\")\n",
    "print(df.info())\n",
    "print(\"\\nSummary Statistics for Numerical Features:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Quantify and visualize the presence of missing data.\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df.isnull(), cbar=False, yticklabels=False)\n",
    "plt.title('Heatmap of Missing Values')\n",
    "plt.show()\n",
    "\n",
    "# Implement data cleansing procedures.\n",
    "# For numerical fields with missing data, imputation with the median is robust to outliers.\n",
    "df['num_pages'].fillna(df['num_pages'].median(), inplace=True)\n",
    "# For categorical fields, imputation with the most frequent category is appropriate.\n",
    "df['language_code'].fillna(df['language_code'].mode()[0], inplace=True)\n",
    "\n",
    "# Engineer the binary target variable based on a percentile threshold.\n",
    "bestseller_threshold = df['ratings_count'].quantile(0.8)\n",
    "df['is_bestseller'] = (df['ratings_count'] >= bestseller_threshold).astype(int)\n",
    "print(f\"Target Variable Distribution:\\n{df['is_bestseller'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Visualize the class balance of the newly created target variable.\n",
    "sns.countplot(x='is_bestseller', data=df)\n",
    "plt.title('Class Distribution of Bestseller Status')\n",
    "plt.xlabel('Bestseller Status (0 = No, 1 = Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# --- PHASE 2: FEATURE SELECTION AND PREPROCESSING ---\n",
    "\n",
    "# Define the initial set of candidate features for the model.\n",
    "feature_columns = ['average_rating', 'num_pages', 'ratings_count', 'language_code']\n",
    "X = df[feature_columns]\n",
    "y = df['is_bestseller']\n",
    "\n",
    "# Preprocess the features: encode categorical variables and scale numerical ones.\n",
    "X_encoded = pd.get_dummies(X, columns=['language_code'], prefix='lang', drop_first=True)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_encoded)\n",
    "# Convert back to DataFrame for clarity in subsequent steps.\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_encoded.columns)\n",
    "\n",
    "# Partition the data into training and testing sets, preserving the class distribution.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Employ statistical backward elimination for feature selection.\n",
    "# Adding a constant term for the intercept in the statistical model.\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "logit_model = sm.Logit(y_train, X_train_sm).fit()\n",
    "print(logit_model.summary())\n",
    "# Based on the p-values in the summary, features with p > 0.05 will be iteratively removed.\n",
    "# Assume the final selected features are: 'average_rating', 'ratings_count', 'lang_eng'\n",
    "selected_features = ['average_rating', 'ratings_count', 'lang_eng']\n",
    "X_train_final = X_train[selected_features]\n",
    "X_test_final = X_test[selected_features]\n",
    "\n",
    "# --- PHASE 3: MODEL IMPLEMENTATION FROM SCRATCH ---\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Computes the sigmoid function, mapping input z to a probability between 0 and 1.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost(y_true, y_pred_prob):\n",
    "    \"\"\"Computes the binary cross-entropy loss.\n",
    "    Clipping is applied to prevent numerical instability from log(0).\"\"\"\n",
    "    y_pred_clipped = np.clip(y_pred_prob, 1e-15, 1 - 1e-15)\n",
    "    return -np.mean(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "\n",
    "def gradient_descent(X, y_true, learning_rate=0.1, iterations=1000):\n",
    "    \"\"\"Performs gradient descent to learn the optimal model parameters.\"\"\"\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros(n)\n",
    "    bias = 0\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Forward pass: compute linear combination and predicted probabilities.\n",
    "        linear_model = np.dot(X, weights) + bias\n",
    "        y_pred_prob = sigmoid(linear_model)\n",
    "        \n",
    "        # Compute and store the cost for monitoring convergence.\n",
    "        cost = compute_cost(y_true, y_pred_prob)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # Backward pass: compute gradients for weights and bias.\n",
    "        gradient_w = (1 / m) * np.dot(X.T, (y_pred_prob - y_true))\n",
    "        gradient_b = (1 / m) * np.sum(y_pred_prob - y_true)\n",
    "        \n",
    "        # Update model parameters.\n",
    "        weights -= learning_rate * gradient_w\n",
    "        bias -= learning_rate * gradient_b\n",
    "        \n",
    "        # Monitor convergence every 100 iterations.\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Cost = {cost:.4f}\")\n",
    "        # Early stopping condition based on minimal cost improvement.\n",
    "        if i > 10 and abs(cost_history[-2] - cost_history[-1]) < 1e-7:\n",
    "            print(f\"Convergence achieved at iteration {i}.\")\n",
    "            break\n",
    "            \n",
    "    return weights, bias, cost_history\n",
    "\n",
    "# Execute the custom training algorithm.\n",
    "trained_weights, trained_bias, historical_cost = gradient_descent(X_train_final.values, y_train.values)\n",
    "\n",
    "# Visualize the convergence of the gradient descent algorithm.\n",
    "plt.plot(historical_cost)\n",
    "plt.title('Convergence Profile of Gradient Descent')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- PHASE 4: MODEL PREDICTION AND EVALUATION ---\n",
    "\n",
    "def predict_classes(X, weights, bias, decision_threshold=0.5):\n",
    "    \"\"\"Generates binary class predictions based on learned parameters and a threshold.\"\"\"\n",
    "    linear_combination = np.dot(X, weights) + bias\n",
    "    probabilities = sigmoid(linear_combination)\n",
    "    return (probabilities >= decision_threshold).astype(int), probabilities\n",
    "\n",
    "# Generate predictions for the test set.\n",
    "y_pred_binary, y_pred_probability = predict_classes(X_test_final.values, trained_weights, trained_bias)\n",
    "\n",
    "# Conduct a comprehensive model evaluation.\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL PERFORMANCE EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_binary):.4f}\\n\")\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_binary))\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "# Generate and plot the Precision-Recall curve.\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_probability)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f'Custom Logistic Model (AUC = {pr_auc:.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
